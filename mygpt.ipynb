{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b27851-9628-45f1-a6a0-33c43f83742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer\n",
    "# Other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9579dca3-84e8-436c-9922-c33a189f7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from source\n",
    "with open('data/training_data.txt', 'r') as file:\n",
    "    raw_text = file.read()\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144a243e-2b47-4049-974a-1861a00246ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.tokenize(raw_text)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394361bc-69c0-4faa-a859-ce15d256063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7892287-8a20-45fc-aa47-53356ccdf64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token and positional embeddings\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, max_len):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_emb = nn.Embedding(max_len, emb_size)\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        return self.token_emb(x) + self.pos_emb(positions)\n",
    "# Example usage:\n",
    "vocab_size = tokenizer.vocab_size\n",
    "emb_size = 256\n",
    "max_len = 512\n",
    "embedding_layer = Embeddings(vocab_size, emb_size, max_len)\n",
    "input_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "inputs_embedded = embedding_layer(input_ids)\n",
    "print(inputs_embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd67014d-83e5-4af9-b8ff-3dad45d06a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define self attention mechanism\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(emb_size, emb_size)\n",
    "        self.key = nn.Linear(emb_size, emb_size)\n",
    "        self.value = nn.Linear(emb_size, emb_size)\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(x.size(-1))\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        return torch.matmul(attn_weights, V)\n",
    "# Example usage:\n",
    "attn = SelfAttention(emb_size)\n",
    "attn_output = attn(inputs_embedded)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6801b4-bb64-4e5e-a265-4a9df7645e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build transformer block (attention + feed-forward + normalization)\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(emb_size)\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_size, ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden, emb_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "# Example usage:\n",
    "transformer_block = TransformerBlock(emb_size, ff_hidden=512)\n",
    "transformed = transformer_block(inputs_embedded)\n",
    "print(transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c729e7-012f-4536-a7e4-0bc439563062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack multiple transformer blocks to form the model\n",
    "class SimpleLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, max_len, num_layers, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, emb_size, max_len)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(emb_size, ff_hidden) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(emb_size, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "# Example usage:\n",
    "model = SimpleLLM(vocab_size, emb_size, max_len, num_layers=2, ff_hidden=512)\n",
    "logits = model(input_ids)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4139cd9-b2d9-40aa-8b68-8dd6b6f111fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f737a9e-08f2-401e-bd4d-1c9a1a6e1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (simplified)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_ids)\n",
    "    # For demonstration, using shifted input as target\n",
    "    target = input_ids[:, 1:]\n",
    "    logits = logits[:, :-1, :]\n",
    "    loss = loss_fn(logits.reshape(-1, vocab_size), target.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
